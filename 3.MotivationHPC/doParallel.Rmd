---
title: "Why should I use HPC?"
author: "Pedro Ojeda"
date: "May 27th, 2020"
output:
  html_document: default
---


## Serial vs. Parallel mode

We will use *doParallel* package and the *iris* database:

```{r}
library(doParallel)
x <- iris[which(iris[,5] != "setosa"), c(1,5)]
trials <- 10000
```

let's take a look at the performance of a logistic regression model in serial mode (1 core):


```{r}
stime <- system.time({
    r <- foreach(icount(trials), .combine=cbind) %do% {
        ind <- sample(100,100, replace=TRUE)
        result1 <- glm(x[ind,2]~x[ind,1], family=binomial(logit))
        coefficients(result1)
    }
})[3]

stime
```

Now, look at the performance using 2 cores:

```{r}
cl <- makeCluster(2)
registerDoParallel(cl)

ptime <- system.time({
    r <- foreach(icount(trials), .combine=cbind) %dopar% {
        ind <- sample(100,100, replace=TRUE)
        result1 <- glm(x[ind,2]~x[ind,1], family=binomial(logit))
        coefficients(result1)
    }
})[3]

ptime
stopCluster(cl)
```

What are the differences between the codes?

Let's try 3 cores:

```{r eval=FALSE}
cl <- makeCluster(3)
registerDoParallel(cl)

ptime <- system.time({
    r <- foreach(icount(trials), .combine=cbind) %dopar% {
        ind <- sample(100,100, replace=TRUE)
        result1 <- glm(x[ind,2]~x[ind,1], family=binomial(logit))
        coefficients(result1)
    }
})[3]

ptime
stopCluster(cl)
```

4 cores:

```{r eval=FALSE}
cl <- makeCluster(4)
registerDoParallel(cl)

ptime <- system.time({
    r <- foreach(icount(trials), .combine=cbind) %dopar% {
        ind <- sample(100,100, replace=TRUE)
        result1 <- glm(x[ind,2]~x[ind,1], family=binomial(logit))
        coefficients(result1)
    }
})[3]

ptime
stopCluster(cl)
```

and finally, 5 cores:

```{r eval=FALSE}
cl <- makeCluster(5)
registerDoParallel(cl)

ptime <- system.time({
    r <- foreach(icount(trials), .combine=cbind) %dopar% {
        ind <- sample(100,100, replace=TRUE)
        result1 <- glm(x[ind,2]~x[ind,1], family=binomial(logit))
        coefficients(result1)
    }
})[3]

ptime
stopCluster(cl)
```

a graphical view of the scaling behavior can be seen in the following plot:

```{r echo=FALSE,warning=FALSE, message=FALSE}
library(tidyverse)

timing <- read.csv('timings.csv', header=TRUE, sep=",")

ggplot(data = timing, mapping = aes(x = Nr.cores, y = Time)) +
  geom_point() + geom_line() + labs(x="Nr. cores", y="Time (sec)")
```


## Is parallel processing always the best alternative?

```{r}
stime <- system.time(
        foreach(i=1:1e4) %do% sqrt(i)
)

stime 

library(doParallel)
cl <- makeCluster(2)
registerDoParallel(cl)
ptime <- system.time( 
        foreach(i=1:1e4) %dopar% sqrt(i)
)

ptime 

stopCluster(cl)
```

the message from this simple calculation is that one always needs to check the performance of the code vs. the number of requested cores.


## References
* https://swcarpentry.github.io/r-novice-inflammation/
* https://www.tutorialspoint.com/r/index.htm
* R High Performance Programming. Aloysius, Lim; William, Tjhi. Packt Publishing, 2015.
